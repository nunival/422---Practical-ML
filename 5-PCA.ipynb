{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5\n",
    "MSDS422 - Fulton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Question:\n",
    "You will compete in the Kaggle.com Digit Recognizer competition which involves classical digit recognition from hand-written images. https://www.kaggle.com/c/digit-recognizer/ (Links to an external site.)\n",
    "\n",
    "Read the competition rules, and download the MNIST training and test set data.This binary classification task is NOT what is required for the current assignment. In this assignment we are asking for a multiclass classifier. The entire MNIST data set will be used for input data. For this assignment, you will develop a classifier that may be used to predict which of the 10 digits is being written.  \n",
    "\n",
    "(1) Begin by fitting a random forest classifier using the full set of 784 explanatory variables and the model training set (train.csv). Record the time it takes to fit the model and then evaluate the model on the test.csv data by submitting to Kaggle.com. Provide your Kaggle.com score and user ID.\n",
    "\n",
    "(2) Execute principal components analysis (PCA) on the combined training and test set data together, generating principal components that represent 95 percent of the variability in the explanatory variables. The number of principal components in the solution should be substantially fewer than the 784 explanatory variables. Record the time it takes to identify the principal components.\n",
    "\n",
    "(3) Using the identified principal components from step (2), use the train.csv to build another random forest classifier. Record the time it takes to fit the model and to evaluate the model on the test.csv data by submitting to Kaggle.com. Provide your Kaggle.com score and user ID.\n",
    "\n",
    "(4) Submit both the RF Classifier and the PCA RF Classifier to Kaggle.com, and report both scores along with your user name.  I MUST have your user name to verify submission status.\n",
    "\n",
    "(5) The experiment we have proposed has a MAJOR design flaw. Identify the flaw. Fix it. Rerun the experiment in a way that is consistent with a training-and-test regimen, and submit this to Kaggle.com. Provide your Kaggle.com score and user ID.\n",
    "\n",
    "Report total elapsed time measures for the training set analysis.  It is sufficient to run a single time-elapsed test for this assignment. In practice, we might consider the possibility of repeated executions of the relevant portions of the programs, much as the Benchmark Example programs do. Some code that might help you with reporting elapsed total time follows. \n",
    "\n",
    "start=datetime.now()\n",
    "rf2.fit(trainimages,labels)\n",
    "end=datetime.now()\n",
    "print(end-start)\n",
    "\n",
    "Relevant scikit-learn documentation includes:\n",
    "\n",
    "    Random Forest Classifier (Links to an external site.)\n",
    "    Metrics Classification Report (Links to an external site.)\n",
    "    Plot Digits Classification (Links to an external site.)\n",
    "    Sklearn Decomposition (Links to an external site.)\n",
    "\n",
    "(Optional reading) If you want to learn about the time it takes to execute individual functions or code segments within Python, this article demonstrates a variety of ways to do it (Links to an external site.).\n",
    "\n",
    "Regarding the F1 score and the evaluation of multiclass classifiers, refer to the literature on information retrieval. See pages 142–145 of this classic reference:\n",
    "\n",
    "Manning, C. D., Raghaven, P., & Schutze, H. (2008). Introduction to information retrieval. Cambridge, UK: Cambridge University Press. [ISBN-13: 978-0521865715]\n",
    "\n",
    "Or see pages 154–158 of the free online version of the book here (Links to an external site.). \n",
    "\n",
    "Additional information about this book is available online here (Links to an external site.).\n",
    "\n",
    " \n",
    "### Programming Notes\n",
    "\n",
    "One of the key parameters in setting up random forests is the number of explanatory variables to include in the individual trees. For this classification problem, I would suggest that we follow the advice of Müller and Guido (2017) and use max_features = 'sqrt' .\n",
    "\n",
    "Regarding the other meta-parameters ...  ensure that bootstrap = True and, given the large number of observations, we might as well keep the default value of n_estimators = 10.\n",
    "\n",
    "Müller, A. C., & Guido, S. (2017). Introduction to machine learning with Python: A guide for data scientists. Sebastopol, CA: O’Reilly. [ISBN-13: 978-1449369415]. Code examples here (Links to an external site.).\n",
    "\n",
    "Another useful reference that discusses the MNIST data set and principal components analysis is:\n",
    "\n",
    "VanderPlas, J. (2017). Python data science handbook: Essential tools for working with data. Sebastopol, CA: O’Reilly [ISBN-13: 978-1491912058]. Python code examples here (Links to an external site.). \n",
    "\n",
    " \n",
    "### Management Problem\n",
    "\n",
    "From a management perspective, the predictive accuracy of models must be weighed against the costs of model development and implementation. Suppose you were the manager of a data science team responsible for implementing models for computer vision (classification of images analogous to the MINST problem). Would you recommend using PCA as a preliminary to machine learning classification? Explain your thinking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Analysis and Insights\n",
    "\n",
    "In step 1 as asked in the initial problem I quickly trained a Random Forest Classifier model on the Kaggle training set, no transformations or fancy parameter tuning. I checked the time it took to train the model. I didn't perform any feature scaling because all of them have the same range of possible values, thus they are already scaled the same.\n",
    "\n",
    "In step 2 the problem asked to combine the training and test Kaggle datasets then train a PCA model to reduce the features down to 95% of the variability. I then fit a new Random Forest Classifier to this trained PCA model. I was then asked to use this same model on the Kaggle test set and check the results. This submission was worse than the first.\n",
    "\n",
    "Step 5 asks what went wrong and to correct it. I knew combining the test and training sets was incorrect. That gives the model information that it shouldn't have, thus leaking data. I corrected this by fitting a PCA model to the training set then used that new dataset to train a new Random Forest Classifier. I then transformed the test data using the PCA model and predicted the results using the RFC. This gave me a score much closer to the first Random Forest.\n",
    "\n",
    "## Results\n",
    "From an accuracy perspective below are the following results:<br>\n",
    "--- Kaggle Scores ------------------------------<br>\n",
    "0.94457 : Initial RFC (step 1) <br>\n",
    "0.88057 : RFC with PCA trained on both datasets (step 3/4)<br>\n",
    "0.91642 : RFC with PCA trained only on test set (step 5)<br>\n",
    "<i>Username: gallagh4</i> <br>\n",
    "\n",
    "I don't find these results too surprising. I expected the accuracy to drop from step 1 to step 3/4. As expected it did not generalize well on data it hasn't seen yet. The difference between step 1 and step 5 isn't too surprising either. Technically we've taken data away in step 5 by performing PCA. By definition we reduced the variability down to 95%. By taking information away I'm not surprised that our score dropped slightly. \n",
    "The scores themselves weren't as important. As mentioned I didn't do any hyperparameter tuning or try any other types of models. The goal was to identify, given the same model, what effect dimensionality reduction has on accuracy (and time).\n",
    "\n",
    "\n",
    "## Analysis\n",
    "I just discussed accuacy and the effect PCA had on it but there is another measurement to consider: time. While we lose some accuracy we gain speed. \n",
    "\n",
    "I started out using <code> n_estimators = 10 </code> in the RFC since that was recommended in the assignment. When I check the times I see the following (in seconds):\n",
    "\n",
    "| Step | PCA Time | Model Time | Total|\n",
    "| --- | --- | --- | --- |\n",
    "| 1 | 0 | 6.05 | 6.05 |\n",
    "| 5 | 5.12 | 1.08 | 6.2 |\n",
    "\n",
    "We see that step 5 (which included PCA) took longer however that's because of the PCA time. There was a very sizable reduction in the model training/fitting time. We can see this much more clearly if we check the times to run for <code> n_estimators = 100 </code>:\n",
    "\n",
    "| Step | PCA Time | Model Time | Total|\n",
    "| --- | --- | --- | --- |\n",
    "| 1 | 0 | 55 | 55 |\n",
    "| 5 | 4.28 | 6.12 | 10.4 |\n",
    "\n",
    "By including PCA we've cut the time down to 20% of what it was. That's a pretty good time savings.\n",
    "\n",
    "For the management question regarding implementing a computer vision problem; it all comes down to the priorities. If model accuracy is super critical then perhaps it is worth not including PCA to get that extra bit of accuracy. However many times in the real world time is a constraint. If that's the case then someone would have to decide whether the reduction in time is worth the slightly lower accuracy. The schedule of how often the model needs to be retrained would influence how important time is. If you have to do it weekly (or even daily) then time plays a much bigger factor. If we replaced the RFC with a neural network that took a lot longer we'd probably be more interested in saving time.\n",
    "\n",
    "In this case the time to run is pretty minimal, we're talking a few seconds of difference. I would exclude the PCA since it isn't doing much to save time on a RFC with n_estimators = 10, but Kaggle competitions are not the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix - Code and Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "#Data Prep\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import time\n",
    "import datetime\n",
    "#Modeling\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#Validation\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data source\n",
    "df = pd.read_csv('./Data/train.csv', sep=',', engine='python')\n",
    "df_test = pd.read_csv('./Data/test.csv', sep=',', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.00000</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.456643</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219286</td>\n",
       "      <td>0.117095</td>\n",
       "      <td>0.059024</td>\n",
       "      <td>0.02019</td>\n",
       "      <td>0.017238</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.887730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.312890</td>\n",
       "      <td>4.633819</td>\n",
       "      <td>3.274488</td>\n",
       "      <td>1.75987</td>\n",
       "      <td>1.894498</td>\n",
       "      <td>0.414264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>253.00000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              label   pixel0   pixel1   pixel2   pixel3   pixel4   pixel5  \\\n",
       "count  42000.000000  42000.0  42000.0  42000.0  42000.0  42000.0  42000.0   \n",
       "mean       4.456643      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "std        2.887730      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "min        0.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "25%        2.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "50%        4.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "75%        7.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "max        9.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "        pixel6   pixel7   pixel8  ...      pixel774      pixel775  \\\n",
       "count  42000.0  42000.0  42000.0  ...  42000.000000  42000.000000   \n",
       "mean       0.0      0.0      0.0  ...      0.219286      0.117095   \n",
       "std        0.0      0.0      0.0  ...      6.312890      4.633819   \n",
       "min        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "25%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "50%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "75%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "max        0.0      0.0      0.0  ...    254.000000    254.000000   \n",
       "\n",
       "           pixel776     pixel777      pixel778      pixel779  pixel780  \\\n",
       "count  42000.000000  42000.00000  42000.000000  42000.000000   42000.0   \n",
       "mean       0.059024      0.02019      0.017238      0.002857       0.0   \n",
       "std        3.274488      1.75987      1.894498      0.414264       0.0   \n",
       "min        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "25%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "50%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "75%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "max      253.000000    253.00000    254.000000     62.000000       0.0   \n",
       "\n",
       "       pixel781  pixel782  pixel783  \n",
       "count   42000.0   42000.0   42000.0  \n",
       "mean        0.0       0.0       0.0  \n",
       "std         0.0       0.0       0.0  \n",
       "min         0.0       0.0       0.0  \n",
       "25%         0.0       0.0       0.0  \n",
       "50%         0.0       0.0       0.0  \n",
       "75%         0.0       0.0       0.0  \n",
       "max         0.0       0.0       0.0  \n",
       "\n",
       "[8 rows x 785 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.\n",
    "Training Random Forest on all features. No transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time:  0:00:06.045460\n"
     ]
    }
   ],
   "source": [
    "# Start time\n",
    "start= datetime.datetime.now()\n",
    "\n",
    "# Training a Random Forest Classification model and checking how long it takes to run\n",
    "rfc = RandomForestClassifier(random_state = 1234, max_features = 'sqrt', bootstrap = True, n_estimators = 10)\n",
    "rfc.fit(df.drop(['label'], axis = 1), df.loc[:,'label'])\n",
    "\n",
    "#Making predictions\n",
    "final_predictions = rfc.predict(df_test)\n",
    "\n",
    "# End Time\n",
    "end= datetime.datetime.now()\n",
    "print(\"Elapsed Time: \",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Packaging submission up for submittal to Kaggle\n",
    "ImageId = df_test.index+1\n",
    "my_solution = pd.DataFrame(final_predictions, ImageId, columns = ['label'])\n",
    "my_solution.to_csv(\"Submissions/submission_1.csv\", index_label = [\"ImageId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle Score: 0.94457<br>\n",
    "User ID: gallagh4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Perform PCA\n",
    "1. Combine Train and Test sets\n",
    "2. Perform PCA to reduce features while still maintaining 75% of the variability\n",
    "3. Record the time it takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending training and test together into one dataset\n",
    "full = df.append(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 785)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirming size looks correct\n",
    "full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time:  0:00:10.334919\n"
     ]
    }
   ],
   "source": [
    "#Recording time to train/fit PCA\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "# Fitting PCA\n",
    "pca = PCA(n_components = 0.95)\n",
    "X_reduced = pca.fit_transform(full.drop(['label'], axis = 1))\n",
    "\n",
    "# Recording End time\n",
    "end = datetime.datetime.now()\n",
    "print(\"Elapsed Time: \",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 154)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We see that the number of features has reduced from 785 to 154. Big drop\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.\n",
    "Building another Random Forest Classifier with the identified principal components on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the test dataset based on the PCA that ran with both test and training\n",
    "X_train_reduced = pca.transform(df.drop(['label'], axis = 1))\n",
    "X_test_reduced = pca.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time:  0:00:02.360381\n"
     ]
    }
   ],
   "source": [
    "start= datetime.datetime.now()\n",
    "\n",
    "# Training a Random Forest Classification model with the PCA training features and checking how long it takes to run\n",
    "rfc = RandomForestClassifier(random_state = 1234, max_features = 'sqrt', bootstrap = True, n_estimators = 10, n_jobs = -1)\n",
    "rfc.fit(X_train_reduced, df.loc[:,'label'])\n",
    "final_predictions = rfc.predict(X_test_reduced)\n",
    "\n",
    "end= datetime.datetime.now()\n",
    "print(\"Elapsed Time: \",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Packaging submission up\n",
    "ImageId = df_test.index+1\n",
    "my_solution = pd.DataFrame(final_predictions, ImageId, columns = ['label'])\n",
    "my_solution.to_csv(\"Submissions/submission_2.csv\", index_label = [\"ImageId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. \n",
    "\n",
    "\n",
    "--- Kaggle Scores --------<br>\n",
    "Initial RFC: 0.94457 <br>\n",
    "RFC with PCA: 0.88057 <br>\n",
    "<i>Username: gallagh4</i><br>\n",
    "\n",
    "\n",
    "This made the score worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem I'm seeing with the previous approach is that we're training on all of the data (including test) then expecting it to generalize to new data. This should be causing data leakage. Let's try this approach again but with performing the PCA on the training set only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training and test sets off of the \n",
    "# training set so we can check the accuracy here\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(['label'], axis= 1),\n",
    "                                                    df.loc[:,'label'], \n",
    "                                                    train_size = 0.8,\n",
    "                                                    random_state = 1776)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time:  0:00:04.513978\n"
     ]
    }
   ],
   "source": [
    "#Recording total time to train/fit PCA on TRAINING set\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "\n",
    "#Initializing PCA\n",
    "pca = PCA(n_components = 0.75)\n",
    "\n",
    "# Fitting and transforming the training set, only transforming the test set\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "X_test_reduced = pca.transform(X_test)\n",
    "\n",
    "# End time\n",
    "end = datetime.datetime.now()\n",
    "print(\"Elapsed Time: \",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_reduced Shape:  (33600, 33)\n",
      "X_test_reduced Shape:  (8400, 33)\n"
     ]
    }
   ],
   "source": [
    "# We see that the number of features has reduced from 785 to 33. An even bigger drop\n",
    "print(\"X_train_reduced Shape: \", X_train_reduced.shape)\n",
    "print(\"X_test_reduced Shape: \", X_test_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time:  0:00:01.062257\n"
     ]
    }
   ],
   "source": [
    "start= datetime.datetime.now()\n",
    "\n",
    "# Training a Random Forest Classification model with the training PCA features and checking how long it takes to run\n",
    "rfc = RandomForestClassifier(random_state = 1234\n",
    "                             ,max_features = 'sqrt'\n",
    "                             ,bootstrap = True,\n",
    "                             n_estimators = 10\n",
    "                             ,n_jobs = -1)\n",
    "\n",
    "\n",
    "rfc.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Creating preditions\n",
    "train_predictions = rfc.predict(X_train_reduced)\n",
    "test_predictions = rfc.predict(X_test_reduced)\n",
    "\n",
    "# End time\n",
    "end= datetime.datetime.now()\n",
    "print(\"Elapsed Time: \",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy Score:  0.9986904761904762\n",
      "Test Accuracy Score:  0.914047619047619\n"
     ]
    }
   ],
   "source": [
    "# Checking accuracy\n",
    "train_acc = accuracy_score(y_train, train_predictions) \n",
    "test_acc = accuracy_score(y_test, test_predictions)\n",
    "print(\"Train Accuracy Score: \", train_acc)\n",
    "print(\"Test Accuracy Score: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transforming and predicting on Kaggle's test set\n",
    "X_test_reduced = pca.transform(df_test)\n",
    "final_predictions = rfc.predict(X_test_reduced)\n",
    "\n",
    "# Packaging submission up\n",
    "ImageId = df_test.index+1\n",
    "my_solution = pd.DataFrame(final_predictions, ImageId, columns = ['label'])\n",
    "my_solution.to_csv(\"Submissions/submission_3_1.csv\", index_label = [\"ImageId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Kaggle Scores ------------------------------<br>\n",
    "0.94457 : Initial RFC (step 1) <br>\n",
    "0.88057 : RFC with PCA trained on both datasets (step 3/4)<br>\n",
    "0.91642 : RFC with PCA trained only on test set (step 5)<br>\n",
    "<i>Username: gallagh4</i> <br>\n",
    "\n",
    "\n",
    "This improved from step 3 where we trained on both the train and test sets. As suspected our model wasn't generalizing well because there was data leakage. While our new model doesn't do as well as the first one (without PCA) it is still pretty close."
   ]
  }
 ],
 "metadata": {
  "CodeCell": {
   "cm_config": {
    "lineWrapping": true
   }
  },
  "MarkdownCell": {
   "cm_config": {
    "lineWrapping": true
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
